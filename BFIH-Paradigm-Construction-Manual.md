# BFIH Paradigm Construction Manual
## Generating Intellectually Honest and Realistically Biased Paradigm Sets

**Version:** 1.0  
**Date:** January 8, 2026  
**For:** AI agents conducting Bayesian Framework for Intellectual Honesty (BFIH) analyses  
**Purpose:** Systematic methodology for constructing paradigm sets with one privileged intellectually honest paradigm and 3-5 realistically biased paradigms

---

## Executive Summary

This manual provides step-by-step instructions for AI agents to generate **paradigm sets** for BFIH analyses. Each set must contain:

1. **One privileged paradigm (K0)**: Maximally intellectually honest, constructed using all forcing functions from the Intellectual Honesty Treatise
2. **3-5 biased paradigms (K1-K5)**: Realistically biased in ways that mirror actual human cognitive limitations, ideological commitments, or disciplinary blind spots

**Core principle**: The privileged paradigm is **NOT neutral** or "unbiased" in the sense of having no perspective. Rather, it is **maximally transparent**, **maximally comprehensive**, and **systematically self-correcting** through forced interrogation of its own blind spots. The biased paradigms represent **partial perspectives** that are intellectually honest within their own frames but lack the forcing function discipline that exposes limitations.

**Key innovation**: By comparing K0 (privileged) against K1-K5 (biased), the analysis reveals:

- Which biases matter empirically (change posteriors significantly)
- Which biases are harmless (converge with K0 despite different priors)
- What K0 can see that biased paradigms cannot (and vice versa)

---

## Table of Contents

1. [Conceptual Foundation](#conceptual-foundation)
2. [The Privileged Paradigm (K0)](#the-privileged-paradigm-k0)
3. [Biased Paradigms (K1-K5)](#biased-paradigms-k1-k5)
4. [Step-by-Step Construction Protocol](#step-by-step-construction-protocol)
5. [Quality Assurance Checks](#quality-assurance-checks)
6. [Worked Example](#worked-example)
7. [Common Pitfalls and Corrections](#common-pitfalls-and-corrections)
8. [Templates and Checklists](#templates-and-checklists)

---

## 1. Conceptual Foundation

### 1.1 What Is a Paradigm?

A **paradigm** in BFIH analysis is a structured interpretive framework consisting of:

| Component | Definition | Example |
|-----:|----------|-----------|
| **Ontology** | What exists? What is real? | Energy as commodity vs. sacred trust |
| **Epistemology** | How do we know? What counts as evidence? | Empirical data only vs. revelation + data |
| **Axiology** | What is valuable? What should we optimize? | Efficiency vs. stewardship |
| **Methodology** | How do we analyze? | Short-term cost-benefit vs. intergenerational impact |
| **Sociology** | Who decides? What institutions matter? | Markets vs. communities vs. traditions |
| **Temporality** | What time horizon? | Quarterly earnings vs. 100-year transitions |

A paradigm is NOT just a "perspective" or "viewpoint"—it is a **systematic interpretive structure** that generates:

- **Prior probabilities** P(H) based on background knowledge
- **Likelihood assessments** P(E|H) based on what counts as evidence
- **Posterior probabilities** P(H|E) via Bayesian updating

### 1.2 Intellectual Honesty vs. Bias

**Intellectual honesty** is NOT the absence of perspective. It is:

1. **Epistemic transparency**: Explicit assumptions, limitations, blind spots
2. **Systematic self-correction**: Forcing functions that interrogate blind spots
3. **Falsifiability**: Clear criteria for what would prove the paradigm wrong
4. **Operational specificity**: Replicable methods for deriving likelihoods
5. **Domain comprehensiveness**: Engagement with all relevant ontological domains

**Bias** is:

1. **Selective attention**: Privileging certain evidence types while ignoring others
2. **Confirmation tendency**: Interpreting ambiguous evidence to support priors
3. **Blind spots**: Systematic inability to see certain hypotheses or mechanisms
4. **Overconfidence**: Assigning extreme priors or likelihoods without justification
5. **Domain narrowness**: Operating within limited ontological scope

**Key insight**: A paradigm can be **internally coherent and intellectually honest within its frame** (e.g., K2 Cultural-Pessimist operating purely within political science) while still being **globally biased** (ignoring theology, history, institutions). The privileged paradigm (K0) must achieve **global intellectual honesty** by systematically checking for and correcting such domain biases.

### 1.3 Why One Privileged Paradigm?

**Rationale for K0 (privileged)**:

1. **Benchmark for comparison**: K0 provides the standard against which to measure how much bias matters
2. **Reveals hidden assumptions**: By maximally interrogating its own assumptions, K0 exposes what other paradigms take for granted
3. **Identifies convergence**: When K1-K5 converge with K0 despite different priors, bias was harmless; when they diverge, bias matters
4. **Demonstrates forcing functions**: K0 shows what becomes possible when intellectual honesty protocols are rigorously applied

**Why not make all paradigms equally honest?**

- **Unrealistic:** Real-world stakeholders, policymakers, and researchers operate with domain biases, ideological commitments, and resource constraints
- **Uninformative:** If all paradigms are maximally honest, we lose the ability to diagnose which biases matter empirically
- **Pedagogically weak:** Showing one privileged paradigm demonstrates what intellectual honesty looks like in practice

### 1.4 Design Constraint: Realistic Bias

**Critical requirement**: Biased paradigms (K1-K5) must be **realistically biased**, not straw men.

**Realistic bias means**:

- Mirrors actual disciplinary boundaries (e.g., economists ignore theology; political scientists underweight technology)
- Reflects genuine ideological commitments (e.g., libertarians prioritize markets; communitarians prioritize traditions)
- Embodies resource constraints (e.g., short-term focused due to electoral cycles or quarterly reporting)
- Represents cognitive limitations (e.g., availability bias, recency bias, overconfidence)

**Unrealistic bias (avoid)**:

- Cartoonish extremism (e.g., "K1 believes all evidence is fake")
- Deliberate dishonesty (e.g., "K2 knowingly cherry-picks data")
- Incompetence (e.g., "K3 cannot do basic statistics")

**Test**: Would an intelligent, well-informed expert in that domain recognize themselves in the paradigm and accept it as a fair representation of their perspective? If not, the bias is unrealistic.

---

## 2. The Privileged Paradigm (K0)

### 2.1 Core Criteria

K0 must satisfy **all five dimensions of intellectual honesty**:

| Dimension | K0 Requirement | Measured By |
|------:|---------------|-------------|
| **Epistemic Transparency** | Explicit assumptions, limitations, blind spots, falsification criteria | Can someone from another paradigm understand and critique it? |
| **Systematic Self-Correction** | All three forcing functions applied (Ontological Scan, Ancestral Check, Paradigm Inversion) | Are blind spots systematically interrogated? |
| **Falsifiability** | Clear predictions and criteria for disconfirmation | What evidence would prove K0 wrong? |
| **Operational Specificity** | Replicable likelihood derivation methods (historical analogy, theory, expert judgment) | Can another agent reproduce the analysis? |
| **Domain Comprehensiveness** | Engages all 7 ontological domains (Biological, Economic, Cultural, Theological, Historical, Institutional, Psychological) | Are all relevant domains checked? |

**Quality threshold**: K0 must score ≥90/100 on the Intellectual Honesty Rubric (adapted from dictionary exemplar rubric):

- Paradigmatic Centrality: ≥36/40
- Operational Specificity: ≥23/25
- Epistemic Transparency: ≥18/20
- Domain Coverage: ≥14/15
- Forcing Function Compliance: +10/10

### 2.2 Construction Protocol for K0

#### Step 1: Ontological Scan (Mandatory)

Check all 7 domains for viable hypotheses:

| Domain | Questions to Ask | Output |
|---:|-----------------|---------|
| **Biological** | Are genetic, physiological, or ecological factors relevant? | List of biological hypotheses OR explicit justification for exclusion |
| **Economic** | Do markets, prices, incentives, trade-offs matter? | List of economic hypotheses |
| **Cultural** | Do values, norms, identities, narratives matter? | List of cultural hypotheses |
| **Theological** | Do religious beliefs, divine commands, sacred obligations matter? | List of theological hypotheses OR justification |
| **Historical** | Do past patterns, path dependence, institutional memory matter? | List of historical hypotheses |
| **Institutional** | Do regulations, governance structures, organizational behaviors matter? | List of institutional hypotheses |
| **Psychological** | Do cognitive biases, motivations, mental models matter? | List of psychological hypotheses |

**Output format**:

```markdown
### Ontological Scan Results

| Domain | Viable Hypotheses | Justification if Excluded |
|--------|------------------|---------------------------|
| Biological | [H_bio_1], [H_bio_2] | N/A |
| Economic | [H_econ_1], [H_econ_2] | N/A |
| ... | ... | ... |
| Theological | None | Target population is explicitly secular; theological obligations have no causal force for non-believers. However, for believers, theological factors would be primary. |

**Domain coverage: X/7**
```

**Critical rule**: You cannot exclude a domain simply because "it's not my expertise" or "it's uncommon in the literature." You must either generate viable hypotheses OR provide **strong empirical justification** (e.g., "target population is non-human" to exclude Theological).

#### Step 2: Ancestral Check (Mandatory)

Identify how the problem was historically solved:

**Questions**:

1. What is the historical baseline for this phenomenon? (pre-modern, early modern, modern)
2. What mechanisms existed before the current system?
3. How long did historical transitions take?
4. What institutional structures governed the domain historically?

**Output format**:

```markdown
### Ancestral Check Results

**Historical baseline**: [Description of pre-current-system state]

**Historical mechanisms**: 
- Mechanism 1: [Description]
- Mechanism 2: [Description]

**Transition timescales**: [X years from System A to System B]

**Institutional structures**: [Governance, property rights, social norms]

**Implication for current analysis**: 
- [How historical baseline calibrates priors]
- [What historical mechanisms are still relevant]
- [What historical timescales suggest about current "success" or "failure" judgments]
```

**Critical rule**: If you propose a "novel" solution or judge current systems as "failures," you must explicitly compare to historical baselines. A 20-year-old system cannot be judged a "failure" if historical transitions took 100 years.

#### Step 3: Paradigm Inversion (Mandatory)

Generate the paradigm maximally opposed to your dominant paradigm:

**Procedure**:

1. Identify your **dominant paradigm family** by checking which assumptions you take as self-evident
2. **Invert each dimension**: Ontology, Epistemology, Axiology, Methodology, Sociology, Temporality
3. From the inverted paradigm, generate **the single strongest hypothesis**
4. Add this hypothesis to your set (even if you assign it low prior)

**Output format**:

```markdown
### Paradigm Inversion Results

**Dominant paradigm** (native assumptions):

- Ontology: [What exists]
- Epistemology: [How we know]
- Axiology: [What is valuable]
- Methodology: [How we analyze]
- Sociology: [Who decides]
- Temporality: [Time horizon]

**Inverted paradigm** (maximally opposed):

- Ontology: [OPPOSITE of above]
- Epistemology: [OPPOSITE]
- Axiology: [OPPOSITE]
- Methodology: [OPPOSITE]
- Sociology: [OPPOSITE]
- Temporality: [OPPOSITE]

**Strongest hypothesis from inverted paradigm**: [H_inv]

**Prior probability assigned**: P(H_inv) = [X] (even if low, must be non-zero)

**Rationale**: [Why this is the strongest claim from the inverted paradigm]
```

**Example**:

- Native: Secular, empiricist, optimize efficiency, short-term, technocratic
- Inverted: Theological, revelatory+empirical, faithfulness to obligations, intergenerational, communal discernment

**Critical rule**: The inversion must be **genuine**, not superficial. "Slightly less secular" is not the inverse of "secular." The inverse of "secular" is "theological." If you feel uncomfortable generating the inverted hypothesis, that discomfort is data—it reveals a blind spot.

#### Step 4: Hypothesis Set Construction

Using results from Steps 1-3, construct a **MECE (Mutually Exclusive, Collectively Exhaustive)** hypothesis set:

**Standard structure**:

- **H0**: Alternative mechanisms/Reframings (expedient catch-all; outcomes yield evidence via different mechanism M3)
- **H1**: Positive claim (proposition true via mechanism M1)
- **H2**: Negative claim (proposition false via mechanism M2)
- **H3**: Conditional/partial claim (proposition true under conditions C)
- **H4+**: Additional specific hypotheses from Ontological Scan or Paradigm Inversion

**MECE Check -- EXTREMELY IMPORTANT**:

- **Mutually Exclusive**: No two hypotheses can both be true simultaneously
- **Collectively Exhaustive**: At least one hypothesis must be true; all logical possibilities covered

**Output format**:

```markdown
### K0 Hypothesis Set

| Hypothesis | Prior P(H) | Rationale for Prior |
|------------|------------|---------------------|
| H0: Alternatives | 0.05 | [Low due to abundant evidence in domains X, Y, Z] |
| H1: [Positive claim] | 0.XX | [Mechanism M1 is supported by evidence E_a, E_b; historical precedent P1] |
| H2: [Negation] | 0.XX | [Mechanism M2 contradicts M1; supported by evidence E_c] |
| H3: [Conditional] | 0.XX | [Domain diversity suggests context-dependent outcomes] |
| H4: [Reframing] | 0.XX | [Historical baseline suggests phenomenon is normal, not anomalous] |
| H5: [Inverted] | 0.XX | [From Paradigm Inversion; relevant to subpopulation S] |

**MECE verification**: ✓ Mutually exclusive (no overlap), ✓ Collectively exhaustive (all logical space covered)
```

#### Step 5: Prior Probability Assignment for Each Paradigm $K_k$

Assign priors P(H|$K_k$) using **explicit methods** conditioned upon the background context of paradigm $K_k$:

- Solely dependent upon **the background context** defined by the paradigm $K_k$.
- Independent of evidence outside of and subsequent to the background context defined by the paradigm $K_k$.

| Method | When to Use | How It Works |
|----------:|:-----------:|------------------|
| **Principle of Indifference** | **NEVER USE** | Always define paradigm and background context so as to supply some knowledge on sensibly assigning priors across hypotheses |
| **Historical Base Rates** | Past frequency data available | P(H) = frequency of outcome in historical cases |
| **Expert Elicitation** | Domain expertise available | P(H) = median expert credence |
| **Theoretical Derivation** | Strong theory exists | P(H) derived from theoretical model parameters |
| **Reference Class** | Analogous cases exist | P(H) = base rate in reference class |

**Output format**:

```markdown
### Prior Assignment Method

**Method used**: [Historical Base Rates]

**Justification**: [Historical energy transitions show X% result in outcome O1, Y% in O2, ...]

**Sensitivity**: Priors varied ±20% in sensitivity analysis; conclusions robust to prior variation within this range.
```

**Critical rule**: You cannot assign priors "intuitively" or "by feel." You must specify the method and justify it with reference to data, theory, or expert judgment. If priors are subjective, acknowledge this explicitly and conduct sensitivity analysis.

#### Step 6: Likelihood Specification for Each Paradigm $K_k$

Given the background context and perspective of paradigm $K_k$, for each evidence item E and hypothesis H, specify P(E|H,$K_k$) using **explicit derivation methods**:

| Method | When to Use | How It Works |
|--------|-------------|--------------|
| **Historical Analogy** | Historical precedents exist | P(E\|H,$K_k$) = frequency of E in historical cases where H was true |
| **Theoretical Prediction** | Theory makes clear predictions | P(E\|H,$K_k$) derived from theory's predictions |
| **Expert Judgment** | Domain-specific knowledge required | P(E\|H,$K_k$) = expert assessment of consistency |
| **Counterfactual Reasoning** | Can simulate H → E | P(E\|H,$K_k$) estimated via mental simulation or modeling |
| **Causal Estimation** | Causal model available | P(E\|H,$K_k$) computed from mechanism of causal model |

**Output format for evidence matrix**:
```markdown
### Evidence Matrix (K0)

| Evidence | P(E\|H1,$K_k$) | P(E\|H2,$K_k$) | P(E\|H3,$K_k$) | ... | Derivation Method |
|----------|---------|---------|---------|-----|-------------------|
| E1: [Description] | 0.XX | 0.XX | 0.XX | ... | Historical analogy: E1 observed in Y% of historical H1 cases |
| E2: [Description] | 0.XX | 0.XX | 0.XX | ... | Theoretical prediction: Theory T predicts E2 under H1 with probability P |
| ... | ... | ... | ... | ... | ... |
```

**Critical rule**: Each likelihood must have an explicit derivation method given the background context and perspective of paradigm $K_k$. You cannot assign P(E|H,$K_k$) = 0.70 without explaining **why 0.70 and not 0.50 or 0.90**. If the derivation is subjective, acknowledge this and vary the likelihood in sensitivity analysis.

#### Step 7: Limitations and Falsification

Explicitly state:

1. **Assumptions**: What must be true for K0 to be valid?
2. **Limitations**: What can K0 not explain or predict?
3. **Blind spots**: What perspectives or evidence types does K0 underweight?
4. **Falsification criteria**: What evidence or outcomes would prove K0 wrong?
5. **Boundary conditions**: When should K0 NOT be used?

**Output format**:
```markdown
### K0 Limitations and Falsification

**Core assumptions**:

1. [Assumption 1: Description + justification]
2. [Assumption 2: Description + justification]

**Known limitations**:

1. [Limitation 1: What K0 cannot explain]
2. [Limitation 2: Domain or time horizon where K0 fails]

**Blind spots**:

1. [Blind spot 1: What K0 systematically underweights]
2. [Blind spot 2: Evidence types K0 is not equipped to handle]

**Falsification criteria**:

1. If [outcome O1] occurs by [date D], then K0 is disconfirmed
2. If evidence shows [pattern P], then K0 hypothesis H_dominant is wrong
3. If [mechanism M] is demonstrated to be absent, then K0 must revise

**When NOT to use K0**:

1. [Scenario 1: e.g., "For non-believers, theological hypotheses have no causal force"]
2. [Scenario 2: e.g., "For short-term optimization, K4 Economic-Realist is more appropriate"]
```

**Critical rule**: If you cannot state what would prove K0 wrong, K0 is unfalsifiable and therefore not intellectually honest. Every paradigm must have clear falsification criteria.

---

## 3. Biased Paradigms (K1-K5)

### 3.1 Core Criteria

Each biased paradigm (K1-K5) must:

1. **Be realistically biased**: Mirror actual human/institutional limitations, not straw men
2. **Be internally coherent**: Make sense within its own frame
3. **Differ meaningfully from K0**: Create different priors or likelihoods that lead to different posteriors
4. **Represent genuine perspectives**: Correspond to actual schools of thought, disciplinary boundaries, or ideological positions
5. **Fail ≥1 forcing function**: Demonstrate how lack of systematic self-correction leads to blind spots

**Quality threshold**: K1-K5 should score 40-75/100 on Intellectual Honesty Rubric:

- 40-55: WEAK (major blind spots, narrow domain coverage)
- 55-70: ACCEPTABLE (some blind spots, moderate coverage)
- 70-75: GOOD (minor blind spots, good coverage but not maximally honest)

**Design principle**: Biased paradigms should be **close enough to K0 to be competitive** (i.e., not obviously wrong) but **different enough to reveal blind spots**. If K1-K5 are identical to K0, the analysis is uninformative. If K1-K5 are cartoonishly biased, the analysis is a straw man.

### 3.2 Types of Realistic Bias

#### Type 1: Domain Bias (Disciplinary Boundary)

**Description**: Paradigm operates entirely within one discipline, ignoring other ontological domains.

**Examples**:

- **Economic Realist**: Sees only markets, prices, incentives; ignores culture, theology, psychology
- **Political Scientist**: Sees only power, institutions, regulations; ignores technology, history, psychology
- **Technologist**: Sees only innovation, efficiency, engineering; ignores politics, culture, institutions

**Implementation**:

- **Ontological Scan**: Systematically ignore 3-5 domains
- **Justification**: "This is outside my disciplinary expertise" (realistic but insufficient for K0)
- **Effect**: Assigns zero or near-zero priors to hypotheses from excluded domains

**Quality check**: Would a disciplinary expert (economist, political scientist, engineer) recognize themselves in this paradigm? If yes, it's realistic.

#### Type 2: Temporal Bias (Time Horizon)

**Description**: Paradigm uses systematically short or long time horizons, leading to different judgments of "success" or "failure."

**Examples**:

- **Quarterly Capitalist**: 3-month horizon; judges investments by immediate returns
- **Electoral Politician**: 2-4 year horizon; judges policies by next election
- **Technocratic Planner**: 10-20 year horizon; judges by medium-term outcomes
- **Intergenerational Steward**: 50-100+ year horizon; judges by long-term sustainability

**Implementation**:

- **Prior assignment**: Heavily weight short-term costs/benefits, discount long-term
- **Likelihood assessment**: Interpret "high current costs" as evidence of failure, ignore historical baselines showing transitions take 100+ years
- **Effect**: Misjudges normal transitional costs as failures

**Quality check**: Does this time horizon correspond to actual institutional constraints (quarterly reporting, electoral cycles, infrastructure lifespans)?

#### Type 3: Ideological Bias (Value Commitment)

**Description**: Paradigm is committed to specific values (liberty, equality, efficiency, tradition) and judges all evidence through that lens.

**Examples**:

- **Libertarian**: Maximizes individual freedom; opposes regulations even if efficient
- **Egalitarian**: Maximizes equality; opposes markets even if efficient
- **Utilitarian**: Maximizes aggregate welfare; ignores distributional effects
- **Communitarian**: Maximizes community cohesion; opposes individualism even if liberating

**Implementation**:

- **Axiology**: Explicit value commitment stated upfront
- **Prior assignment**: High priors for hypotheses consistent with values; low priors for violations
- **Likelihood assessment**: Interprets ambiguous evidence to support values
- **Effect**: Creates ideologically consistent but potentially empirically biased conclusions

**Quality check**: Would an adherent of this ideology recognize themselves in this paradigm and accept it as fair?

#### Type 4: Cognitive Bias (Heuristic Limitation)

**Description**: Paradigm exhibits well-documented cognitive biases (availability, recency, confirmation, overconfidence).

**Examples**:

- **Availability Bias**: Overweights recent or vivid evidence, underweights base rates
- **Recency Bias**: Overweights recent trends, ignores long-term patterns
- **Confirmation Bias**: Seeks evidence supporting priors, avoids contradictory evidence
- **Overconfidence**: Assigns extreme priors or likelihoods without sufficient justification

**Implementation**:

- **Evidence selection**: Systematically cherry-picks evidence consistent with priors
- **Likelihood assessment**: Assigns P(E|H) near 1.0 or 0.0 without justification
- **Prior assignment**: Assigns P(H) = 0.80+ to preferred hypothesis without sensitivity analysis
- **Effect**: Overconfident conclusions with insufficient evidence

**Quality check**: Does this bias correspond to documented patterns in cognitive psychology literature?

#### Type 5: Institutional Bias (Resource Constraint)

**Description**: Paradigm operates under realistic resource constraints (time, data, expertise, tools).

**Examples**:

- **Policy Analyst with 2-week deadline**: Cannot conduct Ancestral Check or Ontological Scan due to time constraints
- **Startup with limited data**: Cannot access historical data or expert elicitation
- **Single-discipline team**: Cannot engage theological or cultural domains due to lack of expertise

**Implementation**:

- **Forcing functions**: Explicitly fails Ontological Scan or Ancestral Check due to "insufficient resources"
- **Prior assignment**: Uses crude heuristics instead of historical base rates
- **Likelihood assessment**: Uses rough estimates instead of rigorous derivation
- **Effect**: Less comprehensive but faster analysis

**Quality check**: Do these constraints mirror real-world policy analysis, business decision-making, or research environments?

### 3.3 Construction Protocol for K1-K5

#### Step 1: Select Bias Type

Choose 1-2 bias types from Section 3.2 for each paradigm:

**Example paradigm set**:

- **K1**: Domain Bias (Economic) + Temporal Bias (Short-term)
- **K2**: Domain Bias (Political) + Ideological Bias (Egalitarian)
- **K3**: Domain Bias (Ecological) + Temporal Bias (Long-term)
- **K4**: Cognitive Bias (Overconfidence) + Institutional Bias (Resource constraint)
- **K5**: Ideological Bias (Libertarian) + Temporal Bias (Short-term)

**Design rule**: No two paradigms should have identical bias combinations. Each must represent a distinct perspective.

#### Step 2: Define Paradigm Frame

For each K1-K5, specify:

```markdown
### K[X]: [Name]

**Bias type**: [Domain/Temporal/Ideological/Cognitive/Institutional]

**Core description**: [1-2 sentence summary of what this paradigm sees and doesn't see]

**Ontology**: [What exists from this perspective]    
**Epistemology**: [What counts as evidence]    
**Axiology**: [What is valuable]    
**Methodology**: [How to analyze]    
**Sociology**: [Who decides]    
**Temporality**: [Time horizon]    

**Forcing function compliance**:

- Ontological Scan: ✗ [Reason for failure]
- Ancestral Check: ✗ [Reason for failure]
- Paradigm Inversion: ✗ [Reason for failure]

**Realistic justification**: [Why this bias is realistic, not a straw man]
```

#### Step 3: Generate Biased Hypothesis Set

Using the paradigm frame, generate hypotheses **as that paradigm would**:

**Example** (K1: Economic + Short-term):

- **H0**: Low prior (0.05) — "Economics provides clear evidence"
- **H1**: High prior (0.40) — "Market failures explain high costs" (short-term focus)
- **H2**: Low prior (0.10) — "Regulatory factors are secondary to markets"
- **H3**: Moderate prior (0.20) — "Conditional on market conditions"
- **H4**: Low prior (0.10)
- **H5**: Low prior (0.10) — "Transitional costs are inefficiencies" (short-term bias)
- **H6**: Zero prior (0.00) — "Theological factors are irrelevant" (domain bias)

**Compare to K0**:

- K0 assigns H5 = 0.35 (transitional costs normal) and H6 = 0.10 (theological factors relevant for believers)
- K1 assigns H5 = 0.10 (short-term bias) and H6 = 0.00 (domain bias)

**Result**: K1 and K0 will differ in posteriors if evidence supports H5 or H6.

#### Step 4: Generate Biased Likelihoods

For each evidence item, assign P(E|H) **as that paradigm would**:

**Example** (K1: Economic + Short-term interpreting E1 = "EU prices 2-4× US"):

| Hypothesis | P(E1\|H) K0 | P(E1\|H) K1 | Rationale for K1 |
|------------|:---:|:---:|----------------------|
| H1 (Failure) | 0.30 | **0.70** | K1 sees high costs as market failure, not transitional (short-term bias) |
| H5 (Transition) | 0.70 | **0.20** | K1 dismisses transitional costs due to short-term focus |
| H6 (Stewardship) | 0.60 | **0.00** | K1 ignores theological factors due to domain bias |

**Result**: K1 will assign higher posterior to H1 (failure) than K0 because K1 systematically overweights short-term costs and ignores long-term transitions.

#### Step 5: Document Bias Explicitly

For each K1-K5, state:

```markdown
### K[X] Bias Documentation

**What K[X] can see**:

- [Strength 1: e.g., "K1 excels at short-term market analysis"]
- [Strength 2: e.g., "K1 rigorously applies economic theory"]

**What K[X] cannot see (blind spots)**:

- [Blind spot 1: e.g., "K1 ignores historical timescales, misjudging 20-year transitions as failures"]
- [Blind spot 2: e.g., "K1 ignores theological factors, missing hypotheses relevant to believers"]

**Why this bias is realistic**:

- [Justification: e.g., "Professional economists typically operate within 1-10 year horizons due to data availability and disciplinary norms"]

**Expected divergence from K0**:

- [Prediction: e.g., "K1 will assign higher posterior to H1 (failure) than K0 due to short-term bias"]
```

---

## 4. Step-by-Step Construction Protocol

### Phase 1: Problem Framing (10 minutes)

**Input**: Topic, proposition, or question to analyze

**Output**: Neutral proposition, domain identification, stakeholder map

#### Step 1.1: Reframe Proposition Neutrally

**Original proposition** (may be biased): [User input]

**Neutral reframing**: [Unbiased version]

**Bias detected in original**: [Description of loaded language, hidden assumptions]

**Example**:

- Original: "Renewable energy has **failed** in Europe"
- Neutral: "European renewable energy adoption has resulted in electricity prices 2-4× higher than US prices"
- Bias detected: "Failed" is a loaded term that presupposes short-term cost minimization as the success criterion

#### Step 1.2: Identify Relevant Domains

Using the Ontological Scan framework, check which of the 7 domains are relevant:

| Domain | Relevant? | Key Mechanisms |
|-------:|:-----:|----------------|
| Biological | Yes/No | [If yes: description] |
| Economic | Yes/No | [If yes: description] |
| Cultural | Yes/No | [If yes: description] |
| Theological | Yes/No | [If yes: description] |
| Historical | Yes/No | [If yes: description] |
| Institutional | Yes/No | [If yes: description] |
| Psychological | Yes/No | [If yes: description] |

**Relevance threshold**: A domain is relevant if hypotheses from that domain could plausibly explain the phenomenon. When in doubt, include it.

#### Step 1.3: Map Stakeholders

Identify who cares about this question and why:

| Stakeholder Group | Perspective | Likely Bias Type |
|--------|--------------------|---------|
| [Group 1] | [What they care about] | [Bias type] |
| [Group 2] | [What they care about] | [Bias type] |
| ... | ... | ... |

**Purpose**: This stakeholder map will inform K1-K5 construction by identifying realistic biases.

---

### Phase 2: Privileged Paradigm Construction (60 minutes)

Follow Section 2.2 (Steps 1-7) to construct K0.

**Checkpoint**: Before proceeding, verify K0 scores ≥90/100 on Intellectual Honesty Rubric.

---

### Phase 3: Biased Paradigm Set Construction (90 minutes)

#### Step 3.1: Select Paradigm Bias Profiles

Using stakeholder map from Phase 1 and bias typology from Section 3.2, select 3-5 distinct bias profiles:

**Template**:
```markdown
| Paradigm | Name | Bias Type 1 | Bias Type 2 | Realistic Justification |
|----------|------|-------------|-------------|------------------------|
| K1 | [Name] | [Type] | [Type] | [Why realistic] |
| K2 | [Name] | [Type] | [Type] | [Why realistic] |
| K3 | [Name] | [Type] | [Type] | [Why realistic] |
| K4 | [Name] | [Type] | [Type] | [Why realistic] |
| K5 | [Name] | [Type] | [Type] | [Why realistic] |
```

**Distinctiveness check**: No two paradigms should have identical bias combinations.

#### Step 3.2: Construct K1-K5

For each paradigm, follow Section 3.3 (Steps 1-5):

1. Define paradigm frame
2. Generate biased hypothesis set (with biased priors)
3. Generate biased likelihoods
4. Document bias explicitly

#### Step 3.3: Verify Realistic Bias

For each K1-K5, ask:

- Would an intelligent expert from this perspective recognize themselves in this paradigm?
- Is this bias grounded in actual disciplinary boundaries, ideological commitments, cognitive science, or institutional constraints?
- Is this paradigm **internally coherent** (makes sense within its own frame)?

If any answer is "no," revise the paradigm.

---

### Phase 4: Comparative Analysis (30 minutes)

#### Step 4.1: Compute Posteriors

For each paradigm (K0-K5), compute Bayesian posteriors:

P(H|E) = [P(E|H) × P(H)] / [Σ P(E|H_i) × P(H_i)]

**Output table**:

```markdown
| Hypothesis | Prior | Posterior K0 | Posterior K1 | Posterior K2 | Posterior K3 | Posterior K4 | Posterior K5 |
|------------|-------|-------------|-------------|-------------|-------------|-------------|-------------|
| H0 | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX |
| H1 | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX |
| ... | ... | ... | ... | ... | ... | ... | ... |
```

#### Step 4.2: Identify Convergence and Divergence

**Convergence**: When K1-K5 posteriors are within ±0.05 of K0 posterior → **bias is empirically harmless** (evidence overwhelms prior bias)

**Divergence**: When K1-K5 posteriors differ from K0 by >0.10 → **bias matters empirically** (prior bias shapes conclusions)

**Output**:
```markdown
### Convergence/Divergence Analysis

**Convergent hypotheses** (bias doesn't matter):

- [H_x]: K0 = 0.XX, K1-K5 = 0.XX ± 0.05 → [Explanation: evidence is so strong it overwhelms bias]

**Divergent hypotheses** (bias matters):

- [H_y]: K0 = 0.XX, K1 = 0.YY (Δ = 0.ZZ) → [Explanation: K1's short-term bias inflates H_y]
- [H_z]: K0 = 0.XX, K2 = 0.YY (Δ = 0.ZZ) → [Explanation: K2's domain bias ignores H_z entirely]
```

#### Step 4.3: Diagnose Blind Spots

For each divergent hypothesis, trace back to the bias that caused it:

**Template**:
```markdown
### Blind Spot Diagnosis: K[X] on H[Y]

**K0 posterior**: P(H_y|E) = 0.XX
**K[X] posterior**: P(H_y|E) = 0.YY
**Divergence**: Δ = 0.ZZ  (0.YY - 0.XX)

**Root cause**:
- **Forcing function failure**: [Which forcing function did K[X] fail?]
  - If Ontological Scan failed: [Which domain was ignored?]
  - If Ancestral Check failed: [Which historical baseline was missing?]
  - If Paradigm Inversion failed: [Which opposite perspective was not considered?]

- **Bias mechanism**: [How did the bias distort priors or likelihoods?]
  - Prior bias: [Did K[X] assign extreme prior to H_y without justification?]
  - Likelihood bias: [Did K[X] overweight/underweight evidence?]

- **Realistic justification**: [Why is this bias realistic, not a straw man?]

**What K[X] cannot see**:

- [Description of blind spot]

**Recommendation for K[X]**:

- [What would K[X] need to do to correct this blind spot?]
```

---

### Phase 5: Final Documentation (30 minutes)

#### Step 5.1: Generate Paradigm Comparison Table

Create a comprehensive comparison of K0-K5 across all dimensions:

```markdown
## Paradigm Comparison Table

| Dimension | K0 (Privileged) | K1 | K2 | K3 | K4 | K5 |
|-----|----|----|----|----|----|----|
| **Name** | [Name] | [Name] | [Name] | [Name] | [Name] | [Name] |
| **Bias Type** | None (maximally honest) | [Type] | [Type] | [Type] | [Type] | [Type] |
| **Ontological Scan** | ✓ Pass (6.5/7 domains) | ✗ Fail ([reason]) | ✗ Fail ([reason]) | ✗ Fail ([reason]) | ✗ Fail ([reason]) | ✗ Fail ([reason]) |
| **Ancestral Check** | ✓ Pass ([historical baseline]) | ✗ Fail ([reason]) | ✗ Fail ([reason]) | ✗ Fail ([reason]) | ✗ Fail ([reason]) | ✗ Fail ([reason]) |
| **Paradigm Inversion** | ✓ Pass ([inverted paradigm]) | ✗ Fail ([reason]) | ✗ Fail ([reason]) | ✗ Fail ([reason]) | ✗ Fail ([reason]) | ✗ Fail ([reason]) |
| **Domain Coverage** | [X/7 domains] | [X/7 domains] | [X/7 domains] | [X/7 domains] | [X/7 domains] | [X/7 domains] |
| **Time Horizon** | [X years] | [X years] | [X years] | [X years] | [X years] | [X years] |
| **Dominant Hypothesis** | [H_x] (posterior = 0.XX) | [H_y] (0.XX) | [H_z] (0.XX) | [H_w] (0.XX) | [H_v] (0.XX) | [H_u] (0.XX) |
| **Intellectual Honesty Score** | 90-105/100 | 40-55/100 | 55-70/100 | 55-70/100 | 40-55/100 | 55-70/100 |
```

#### Step 5.2: Write Executive Summary

Synthesize the analysis for decision-makers:

```markdown
## Executive Summary

**Privileged Paradigm (K0)**: [Name]

- **Conclusion**: [Dominant hypothesis with posterior]
- **Key strength**: [What K0 sees that others miss]
- **Limitations**: [What K0 cannot explain]

**Biased Paradigms (K1-K5)**: [Summary]

- **Convergence**: [Which biases turned out to be harmless]
- **Divergence**: [Which biases distorted conclusions significantly]

**Recommendation**:

- For [Stakeholder X] with [bias type Y]: Use paradigm [K_i] with caution, noting blind spots [B1, B2]
- For long-term decision-making: Prioritize K0 conclusions
- For short-term optimization: [Specific paradigm] may be appropriate

**Key insight**: [What comparing K0 to K1-K5 reveals about the problem]
```

---

## 5. Quality Assurance Checks

### 5.1 K0 Quality Checklist

Before finalizing K0, verify:

- [ ] **Ontological Scan complete**: All 7 domains checked; exclusions justified explicitly
- [ ] **Ancestral Check complete**: Historical baseline identified; transition timescales noted
- [ ] **Paradigm Inversion complete**: Inverted paradigm generated; strongest hypothesis from inverse included in hypothesis set
- [ ] **MECE hypothesis set**: Mutually exclusive, collectively exhaustive
- [ ] **Prior justification**: Each prior has explicit derivation method (historical base rate, expert elicitation, principle of indifference, etc.)
- [ ] **Likelihood justification**: Each P(E|H) has explicit derivation method (historical analogy, theory, expert judgment, etc.)
- [ ] **Limitations stated**: Assumptions, blind spots, falsification criteria, boundary conditions all explicit
- [ ] **Falsifiable**: Clear predictions that would disconfirm K0
- [ ] **Sensitivity analysis**: Priors and likelihoods varied ±20%; conclusions robust or sensitivity documented
- [ ] **Intellectual Honesty Score**: ≥90/100 on adapted rubric

### 5.2 K1-K5 Quality Checklist

For each biased paradigm, verify:

- [ ] **Bias type identified**: Clear specification of domain/temporal/ideological/cognitive/institutional bias
- [ ] **Realistic justification**: Bias mirrors actual human/institutional limitations, not straw man
- [ ] **Internal coherence**: Paradigm makes sense within its own frame
- [ ] **Distinctiveness**: No two paradigms have identical bias combinations
- [ ] **Forcing function failure documented**: Which forcing functions failed and why
- [ ] **Blind spots explicit**: What this paradigm cannot see is stated clearly
- [ ] **Expected divergence predicted**: Before computing posteriors, predict how this paradigm will differ from K0
- [ ] **Intellectual Honesty Score**: 40-75/100 (lower than K0 but not cartoonishly biased)

### 5.3 System-Level Checks

After constructing full paradigm set (K0-K5), verify:

- [ ] **Distinctiveness**: Posteriors differ across paradigms by ≥0.10 for at least one hypothesis (if all paradigms converge, bias is empirically irrelevant)
- [ ] **Realism**: Each K1-K5 corresponds to actual stakeholder groups, disciplinary boundaries, or cognitive limitations
- [ ] **Coverage**: Paradigm set collectively represents major perspectives on the question
- [ ] **K0 privilege is earned**: K0 scores significantly higher (≥15 points) than K1-K5 on Intellectual Honesty Rubric due to forcing function compliance
- [ ] **Blind spot diagnosis complete**: For each divergent hypothesis, root cause traced to specific forcing function failure
- [ ] **Pedagogical value**: Comparing K0 to K1-K5 reveals insights that K0 alone would not show

---

## 6. Worked Example

### Problem: "Does social media cause teen mental health decline?"

#### Phase 1: Problem Framing

**Original proposition**: "Social media is destroying teen mental health"

**Neutral reframing**: "Has adolescent mental health declined since 2010, and if so, what is the causal role of social media adoption?"

**Bias detected**: "Destroying" presupposes causation and high effect size; original proposition lacks consideration of alternative causes

**Relevant domains**:

- Biological: Yes (neurodevelopment, sleep, stress)
- Economic: Yes (attention economy, monetization incentives)
- Cultural: Yes (norms around screen time, parenting styles)
- Theological: Partial (religious communities have different usage patterns)
- Historical: Yes (comparison to past media panics: TV, video games)
- Institutional: Yes (platform governance, school policies)
- Psychological: Yes (FOMO, social comparison, addictive design)

**Stakeholder map**:

- Parents: Concerned, temporal bias (short-term child wellbeing)
- Platform companies: Economic bias (prioritize engagement over wellbeing)
- Psychologists: Domain bias (focus on individual psychology, underweight sociology)
- Policy makers: Institutional bias (resource-constrained, need quick answers)
- Teens: Experiential bias (overweight personal experience, underweight population data)

---

#### Phase 2: K0 Construction

**K0: Historical-Developmental Integrationist**

**Ontological Scan results**:

| Domain | Viable Hypotheses | Coverage |
|--------|--------------------------|---|
| Biological | H_bio: Sleep disruption → mental health decline | ✓ |
| Economic | H_econ: Attention economy incentivizes addictive design → overuse → decline | ✓ |
| Cultural | H_cult: Decline in in-person community → loneliness → decline | ✓ |
| Theological | H_theo: Decline in religious participation (correlated with social media rise) → loss of meaning → decline | ✓ |
| Historical | H_hist: Moral panic; no causal effect (cf. TV, video games, novels) | ✓ |
| Institutional | H_inst: School policies on phone use mediate effects | ✓ |
| Psychological | H_psych: Social comparison → low self-esteem → decline | ✓ |

**Domain coverage: 7/7**

**Ancestral Check**:

- **Historical baseline**: Pre-2007 (pre-iPhone): Teens experienced mental health issues, but rates were stable or declining 1990-2007
- **Historical mechanisms**: In-person social interaction, phone calls, limited screen time
- **Comparable moral panics**: 1950s TV ("destroying childhood"), 1980s video games ("causing violence"), 1700s novels ("corrupting youth")
- **Pattern**: Past panics proved largely unfounded; modest effects found but not catastrophic
- **Implication**: Prior for "catastrophic effect" should be LOW based on historical base rate of moral panics being overblown

**Paradigm Inversion**:

- **Native paradigm**: Social science, causal inference, secular, harm-focused
- **Inverted paradigm**: Theological, virtue ethics, meaning-focused
  - Inverted hypothesis: **H_theo_inv**: Social media is not the cause; the cause is **loss of transcendent meaning** (decline in religious participation, rise of nihilism). Social media is a symptom, not a cause—teens fill the meaning void with digital engagement.
- **Included in hypothesis set**: H5 (below) with P(H5) = 0.10

**K0 Hypothesis Set**:

| Hypothesis | Prior P(H) | Rationale |
|------------|------------|-----------|
| H0: Catch-All | 0.05 | Abundant evidence from psychology, sociology, economics |
| H1: Strong causal effect (social media → decline) | 0.20 | Plausible but historical base rate of moral panics suggests caution |
| H2: No causal effect (spurious correlation) | 0.15 | Possible; decline may have other causes (economic anxiety, climate change) |
| H3: Conditional effect (harmful for vulnerable subgroups only) | 0.30 | **Highest prior**: Evidence suggests girls, heavy users, those with pre-existing vulnerabilities most affected |
| H4: Reverse causation (decline → more social media use as coping) | 0.10 | Plausible mechanism |
| H5: Mediating factor (loss of meaning → social media + decline) | 0.10 | From Paradigm Inversion |
| H6: Bidirectional (feedback loop) | 0.10 | Social media and decline mutually reinforce |

**MECE check**: ✓ Covers strong effect, no effect, conditional, reverse, mediating, bidirectional

**Likelihood derivation example** (E1 = "Mental health declined 2010-2020"):

| Hypothesis | P(E1\|H) | Derivation Method |
|------------|---------|-------------------|
| H1 | 0.80 | Theoretical prediction: If social media causes decline, we expect decline post-adoption |
| H2 | 0.20 | Counterfactual: If no effect, decline would need alternative cause; plausible but less likely |
| H3 | 0.70 | Partial effect: decline concentrated in heavy users and vulnerable groups |
| H4 | 0.60 | Reverse causation: decline → more use, so we'd still see decline |
| H5 | 0.80 | Mediating factor: both social media and decline rise due to third cause |
| H6 | 0.85 | Bidirectional: feedback loop produces strong decline |

**Limitations**:

- Cannot definitively establish causation with observational data
- Aggregates diverse platforms (Instagram ≠ TikTok ≠ YouTube)
- Teen self-reports may be biased
- Historical comparison assumes past moral panics are analogous (may not be)

**Falsification criteria**:

- If randomized controlled trial shows no effect, H1/H3 disconfirmed
- If mental health decline preceded social media adoption (timing wrong), H1 disconfirmed
- If decline is identical across users and non-users, H1/H3 disconfirmed

---

#### Phase 3: K1-K5 Construction

**K1: Psychological Determinist** (Domain Bias + Cognitive Bias: Confirmation)

- **Bias**: Focuses only on individual psychology; ignores sociology, history, theology
- **Ontological Scan**: Fails (only Psychological domain covered)
- **Ancestral Check**: Fails (no historical comparison to past moral panics)
- **Hypothesis set**: Assigns H1 (strong causal effect) = 0.60 prior (overconfident)
- **Expected divergence**: K1 will have higher posterior for H1 than K0

**K2: Tech Industry Skeptic** (Ideological Bias: Anti-corporate)

- **Bias**: Assumes platforms prioritize profit over wellbeing; interprets all evidence as confirming harm
- **Ontological Scan**: Partial (covers Economic, Institutional, but ignores Theological, Historical)
- **Ancestral Check**: Fails
- **Hypothesis set**: Assigns H1 = 0.50, H2 = 0.05 (low prior for "no effect")
- **Expected divergence**: K2 will have higher posterior for H1 than K0

**K3: Tech Industry Defender** (Ideological Bias: Pro-technology + Institutional Bias: Conflict of interest)

- **Bias**: Assumes technology is net positive; motivated reasoning to defend platforms
- **Ontological Scan**: Partial (covers Psychological, Economic, but ignores Cultural, Theological)
- **Ancestral Check**: Partial (cites past moral panics but cherry-picks)
- **Hypothesis set**: Assigns H2 (no effect) = 0.50, H1 = 0.05 (low prior for harm)
- **Expected divergence**: K3 will have higher posterior for H2 than K0

**K4: Parent with Short-term Focus** (Temporal Bias + Cognitive Bias: Availability)

- **Bias**: Sees child's distress daily; overweights vivid recent evidence, underweights base rates
- **Ontological Scan**: Fails (focuses only on immediate psychological harm)
- **Ancestral Check**: Fails (no historical context)
- **Hypothesis set**: Assigns H1 = 0.70 (overconfident based on anecdotal evidence)
- **Expected divergence**: K4 will have much higher posterior for H1 than K0

**K5: Historical Skeptic** (Domain Bias: History + Temporal Bias: Long-term)

- **Bias**: Overweights historical analogies (TV, video games); assumes current panic is identical to past
- **Ontological Scan**: Partial (covers Historical, Cultural, but underweights Psychological)
- **Ancestral Check**: ✓ Pass (uses historical comparison)
- **Hypothesis set**: Assigns H2 (no effect) = 0.40 (based on past moral panics being overblown)
- **Expected divergence**: K5 will have higher posterior for H2 than K0

---

#### Phase 4: Comparative Analysis

**Posteriors (after evidence)**:

| Hypothesis | K0 | K1 | K2 | K3 | K4 | K5 |
|------------|----|----|----|----|----|----|
| H1 (Strong effect) | 0.25 | 0.55 | 0.50 | 0.10 | 0.65 | 0.15 |
| H2 (No effect) | 0.10 | 0.05 | 0.05 | 0.45 | 0.05 | 0.40 |
| H3 (Conditional) | **0.45** | 0.25 | 0.30 | 0.30 | 0.20 | 0.25 |
| H4 (Reverse) | 0.05 | 0.05 | 0.05 | 0.05 | 0.03 | 0.05 |
| H5 (Mediating) | 0.10 | 0.05 | 0.05 | 0.05 | 0.05 | 0.10 |
| H6 (Bidirectional) | 0.05 | 0.05 | 0.05 | 0.05 | 0.02 | 0.05 |

**Convergence/Divergence**:

- **Divergent**: H1, H2, H3 (all paradigms differ significantly)
- **Convergent**: H4, H5, H6 (all paradigms agree these are low probability)

**Blind spot diagnosis**:

**K1 (Psychological Determinist)**: 

- Assigns H1 = 0.55 (vs. K0 = 0.25)
- **Root cause**: Failed Ontological Scan (ignored Historical domain showing past moral panics); failed Ancestral Check (no base rate comparison)
- **Bias mechanism**: Overweights individual psychology evidence, underweights sociological/historical context
- **Blind spot**: Cannot see that conditional effects (H3) better explain data than universal strong effects (H1)

**K3 (Tech Industry Defender)**:

- Assigns H2 = 0.45 (vs. K0 = 0.10)
- **Root cause**: Ideological bias (motivated reasoning); cherry-picked historical analogies while ignoring psychological evidence
- **Bias mechanism**: Underweights evidence of harm from psychology literature
- **Blind spot**: Cannot see conditional effects (H3) because ideological commitment to "no harm" narrative

**K4 (Parent with Short-term Focus)**:

- Assigns H1 = 0.65 (vs. K0 = 0.25)
- **Root cause**: Cognitive bias (availability), Temporal bias (short-term focus)
- **Bias mechanism**: Overweights vivid anecdotal evidence from own child, underweights population base rates
- **Blind spot**: Cannot see that individual experience may not generalize (H3 conditional effect)

**Key insight**: K0's conclusion (H3 conditional effect, 0.45 posterior) is more nuanced than any biased paradigm. K1/K2/K4 overestimate harm; K3/K5 underestimate harm. Only K0, with full Ontological Scan and Ancestral Check, identifies the conditional nature of the effect.

---

## 7. Common Pitfalls and Corrections

### Pitfall 1: K0 Is "Neutral" or "Unbiased"

**Error**: Treating K0 as having no perspective or being "view from nowhere"

**Correction**: K0 is NOT neutral—it has a perspective (Historical-Developmental Integrationist in the example). K0 is privileged because it **systematically interrogates its own blind spots** via forcing functions, not because it lacks a perspective.

**Test**: If K0 has no assumptions, it cannot generate priors. K0 must have assumptions; the difference is K0 **states them explicitly and tests them systematically**.

### Pitfall 2: Biased Paradigms Are Straw Men

**Error**: Making K1-K5 cartoonishly biased (e.g., "K1 ignores all evidence")

**Correction**: Biased paradigms must be **realistic**. Ask: "Would an intelligent expert from this perspective recognize themselves and accept this as fair?"

**Test**: If K1-K5 score <40/100 on Intellectual Honesty Rubric, they are likely straw men. Realistic bias should score 40-75/100 (weak to acceptable, not incompetent).

### Pitfall 3: All Paradigms Converge

**Error**: K0-K5 all have identical posteriors

**Diagnosis**: Either (1) evidence is so strong it overwhelms all biases (rare), or (2) paradigms are not sufficiently distinct

**Correction**: 

- Check that K1-K5 have genuinely different priors or likelihood assessments
- If convergence persists, acknowledge that bias is empirically irrelevant for this question (this is a valid finding!)

### Pitfall 4: K0 Fails Forcing Functions

**Error**: K0 does not complete Ontological Scan, Ancestral Check, or Paradigm Inversion

**Diagnosis**: K0 is not actually privileged; it's just another biased paradigm labeled differently

**Correction**: Go back to Phase 2 and rigorously apply all forcing functions. If K0 cannot pass all three, it is not intellectually honest enough to be privileged.

### Pitfall 5: Forcing Functions Are Checked Off Without Substance

**Error**: "Ontological Scan: ✓ Complete" but only 3/7 domains actually covered

**Diagnosis**: Box-checking without genuine engagement

**Correction**: Forcing functions must produce **substantive outputs**:
- Ontological Scan → List of hypotheses OR explicit justifications for exclusion
- Ancestral Check → Historical baseline with timescales, mechanisms, base rates
- Paradigm Inversion → Actual inverted paradigm with strongest hypothesis from that perspective

**Test**: Could someone else **reproduce** your forcing function process? If not, it's not operationally specified.

### Pitfall 6: Priors or Likelihoods Are Unjustified

**Error**: "P(H1) = 0.40" with no explanation

**Diagnosis**: Ad hoc assignment; not replicable

**Correction**: Every prior and likelihood must have an **explicit derivation method**:

- Historical base rate
- Expert elicitation
- Theoretical prediction
- Principle of indifference
- Reference class

**Test**: If someone asks "Why 0.40 and not 0.30 or 0.50?" can you give a non-arbitrary answer?

### Pitfall 7: No Sensitivity Analysis

**Error**: Assigning precise priors (e.g., P(H1) = 0.382) without testing robustness

**Diagnosis**: False precision; overfitting

**Correction**: Always conduct sensitivity analysis:

- Vary priors ±20%
- Vary likelihoods ±20%
- Check if conclusions are robust or change dramatically

**Test**: If tiny changes in priors flip conclusions, the analysis is fragile. Acknowledge this explicitly.

### Pitfall 8: Paradigms Don't Map to Real Stakeholders

**Error**: Creating paradigms based on abstract categories rather than actual perspectives

**Diagnosis**: Paradigms feel artificial; no real-world stakeholder would recognize themselves

**Correction**: Ground paradigms in real stakeholders (from Phase 1 stakeholder map):

- Who cares about this question?
- What are their actual constraints (time, resources, ideology, discipline)?
- What would they naturally focus on or ignore?

**Test**: Can you name a real organization, profession, or movement that would adopt each paradigm?

---

## 8. Templates and Checklists

### 8.1 K0 Construction Template

```markdown
# K0: [Name] (Privileged Paradigm)

## Forcing Function 1: Ontological Scan

| Domain | Viable Hypotheses | Justification if Excluded |
|--------|------------------|---------------------------|
| Biological | | |
| Economic | | |
| Cultural | | |
| Theological | | |
| Historical | | |
| Institutional | | |
| Psychological | | |

**Domain coverage: X/7**

## Forcing Function 2: Ancestral Check

**Historical baseline**: [Description]

**Historical mechanisms**: 

1. [Mechanism 1]
2. [Mechanism 2]

**Transition timescales**: [X years]

**Implication for priors**: [How history calibrates priors]

## Forcing Function 3: Paradigm Inversion

**Native paradigm**:

- Ontology: [What exists]
- Epistemology: [How we know]
- Axiology: [What is valuable]
- Methodology: [How we analyze]
- Sociology: [Who decides]
- Temporality: [Time horizon]

**Inverted paradigm**:

- Ontology: [OPPOSITE]
- Epistemology: [OPPOSITE]
- Axiology: [OPPOSITE]
- Methodology: [OPPOSITE]
- Sociology: [OPPOSITE]
- Temporality: [OPPOSITE]

**Strongest hypothesis from inverted paradigm**: [H_inv]
**Prior assigned**: P(H_inv) = [X]

## Hypothesis Set

| Hypothesis | Prior P(H) | Rationale | Derivation Method |
|------------|------------|-----------|-------------------|
| H0: Unknown | | | |
| H1: [Strong claim] | | | |
| H2: [Negation] | | | |
| H3: [Conditional] | | | |
| H4: [Alternative] | | | |
| H5: [Reframing] | | | |
| H6: [Inverted] | | | |

**MECE check**: ✓ Mutually exclusive, ✓ Collectively exhaustive

## Evidence Matrix

| Evidence | P(E\|H1) | P(E\|H2) | P(E\|H3) | P(E\|H4) | P(E\|H5) | P(E\|H6) | Derivation Method |
|----------|---------|---------|---------|---------|---------|---------|-------------------|
| E1: [Description] | | | | | | | |
| E2: [Description] | | | | | | | |

## Limitations and Falsification

**Core assumptions**:
1. [Assumption 1]
2. [Assumption 2]

**Known limitations**:
1. [Limitation 1]
2. [Limitation 2]

**Blind spots**:
1. [Blind spot 1]
2. [Blind spot 2]

**Falsification criteria**:
1. If [outcome O1], then K0 is disconfirmed
2. If [evidence E_x], then K0 hypothesis H_dominant is wrong

**When NOT to use K0**:
1. [Scenario 1]
2. [Scenario 2]

## Intellectual Honesty Score

| Dimension | Score | Justification |
|-----------|-------|---------------|
| Paradigmatic Centrality | /40 | |
| Operational Specificity | /25 | |
| Epistemic Transparency | /20 | |
| Domain Coverage | /15 | |
| Forcing Function Compliance | /10 | |
| **TOTAL** | /100 | |

**Target: ≥90/100**
```

### 8.2 K1-K5 Construction Template

```markdown
# K[X]: [Name] (Biased Paradigm)

## Bias Profile

**Bias Type 1**: [Domain/Temporal/Ideological/Cognitive/Institutional]
**Bias Type 2**: [Domain/Temporal/Ideological/Cognitive/Institutional]

**Realistic justification**: [Why this bias mirrors actual human/institutional limitations]

**Stakeholder represented**: [Which stakeholder group would adopt this paradigm?]

## Paradigm Frame

**Ontology**: [What exists from this perspective]
**Epistemology**: [What counts as evidence]
**Axiology**: [What is valuable]
**Methodology**: [How to analyze]
**Sociology**: [Who decides]
**Temporality**: [Time horizon]

## Forcing Function Compliance

- **Ontological Scan**: ✗ Fail — [Reason: e.g., "Ignores Theological, Historical, Psychological domains due to disciplinary boundary"]
- **Ancestral Check**: ✗ Fail — [Reason: e.g., "No historical baseline; short-term focus"]
- **Paradigm Inversion**: ✗ Fail — [Reason: e.g., "Does not consider opposite perspective"]

## Hypothesis Set (Biased Priors)

| Hypothesis | Prior P(H) | Rationale | Comparison to K0 |
|------------|------------|-----------|------------------|
| H0: Unknown | | | K0 = [X] |
| H1: [Strong claim] | | | K0 = [X], Δ = [difference] |
| H2: [Negation] | | | K0 = [X], Δ = [difference] |
| H3: [Conditional] | | | K0 = [X], Δ = [difference] |
| H4: [Alternative] | | | K0 = [X], Δ = [difference] |
| H5: [Reframing] | | | K0 = [X], Δ = [difference] |
| H6: [Inverted] | | | K0 = [X], Δ = [difference] |

**Expected divergence from K0**: [Prediction: Which hypotheses will have higher/lower posteriors than K0?]

## Evidence Matrix (Biased Likelihoods)

| Evidence | P(E\|H1) | P(E\|H2) | ... | How Bias Distorts Assessment |
|----------|---------|---------|-----|------------------------------|
| E1: [Description] | | | | [e.g., "Short-term bias overweights immediate costs"] |
| E2: [Description] | | | | [e.g., "Domain bias ignores historical precedent"] |

## Bias Documentation

**What K[X] can see**:
1. [Strength 1]
2. [Strength 2]

**What K[X] cannot see (blind spots)**:
1. [Blind spot 1: specific domain/mechanism/evidence type ignored]
2. [Blind spot 2: systematic distortion]

**Why this bias is realistic**:
[Explanation: e.g., "Professional economists typically operate within 1-10 year horizons due to data availability and publication cycles. This paradigm reflects that constraint."]

## Intellectual Honesty Score

**Target: 40-75/100** (lower than K0 but not cartoonish)

| Dimension | Score | Justification |
|-----------|-------|---------------|
| Paradigmatic Centrality | /40 | |
| Operational Specificity | /25 | |
| Epistemic Transparency | /20 | |
| Domain Coverage | /15 | |
| Forcing Function Compliance | /10 | |
| **TOTAL** | /100 | |
```

### 8.3 Quick Reference Checklist

#### Before Starting
- [ ] Proposition is reframed neutrally
- [ ] Relevant domains identified (7-domain scan)
- [ ] Stakeholder map created

#### K0 Construction
- [ ] Ontological Scan complete (all 7 domains checked)
- [ ] Ancestral Check complete (historical baseline identified)
- [ ] Paradigm Inversion complete (inverted hypothesis generated)
- [ ] MECE hypothesis set constructed
- [ ] Priors have explicit derivation methods
- [ ] Likelihoods have explicit derivation methods
- [ ] Limitations and falsification criteria stated
- [ ] Sensitivity analysis conducted
- [ ] Intellectual Honesty Score ≥90/100

#### K1-K5 Construction
- [ ] 3-5 distinct bias profiles selected
- [ ] Each paradigm has realistic justification
- [ ] Each paradigm maps to real stakeholders
- [ ] Forcing function failures documented
- [ ] Blind spots explicitly stated
- [ ] Expected divergence from K0 predicted
- [ ] Intellectual Honesty Scores 40-75/100

#### Comparative Analysis
- [ ] Posteriors computed for all paradigms
- [ ] Convergence/divergence identified
- [ ] Blind spot diagnosis complete (root causes traced)
- [ ] Paradigm comparison table generated
- [ ] Executive summary written

#### Final Quality Check
- [ ] K0 is not a straw man or "view from nowhere"
- [ ] K1-K5 are realistic, not cartoonish
- [ ] Paradigm set collectively represents major perspectives
- [ ] K0 privilege is earned through forcing function compliance
- [ ] Analysis reveals insights beyond what K0 alone would show

---

## Conclusion

This manual provides a systematic, replicable method for AI agents to generate **intellectually honest paradigm sets** for BFIH analyses. The key innovation is the **privileged paradigm (K0)** constructed via mandatory forcing functions, which serves as a benchmark against which to measure the empirical impact of realistic biases in K1-K5.

**Core principles**:

1. **K0 is not neutral**—it has a perspective, but is impeccably intellectually honest and systematically interrogates its own blind spots
2. **K1-K5 are not straw men**—they mirror actual human/institutional limitations
3. **Comparison reveals insight**—divergence shows which biases matter empirically; convergence shows which biases are harmless
4. **Forcing functions are mandatory**—Ontological Scan, Ancestral Check, Paradigm Inversion are not optional for K0

By following this protocol, AI agents can generate paradigm sets that are:

- **Rigorous**: K0 meets highest standards of intellectual honesty
- **Realistic**: K1-K5 reflect actual cognitive/ideological/institutional constraints
- **Informative**: Comparison reveals which biases distort conclusions and which are empirically benign
- **Pedagogical**: Demonstrates the value of forcing functions by showing what K0 can see that biased paradigms cannot

**Usage**: Apply this manual whenever conducting a BFIH analysis that requires multiple paradigms. The privileged paradigm (K0) provides the intellectually honest baseline; the biased paradigms (K1-K5) reveal which biases matter for the question at hand.
